{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0497b566-2041-40c8-9509-2e3a3e6da3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to frequent_itemsets.csv, association_rules.csv, and associations.csv\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class ECLAT:\n",
    "    def __init__(self, min_support):\n",
    "        self.min_support = min_support\n",
    "        self.itemsets = defaultdict(list)\n",
    "        self.freq_itemsets = {}\n",
    "        self.associations = []\n",
    "\n",
    "    def fit(self, transactions):\n",
    "        self.transactions = transactions\n",
    "        self._create_itemsets()\n",
    "        self._find_frequent_itemsets()\n",
    "        self._generate_associations()\n",
    "\n",
    "    def _create_itemsets(self):\n",
    "        for tid, transaction in enumerate(self.transactions):\n",
    "            for item in transaction:\n",
    "                self.itemsets[item].append(tid)\n",
    "\n",
    "    def _find_frequent_itemsets(self):\n",
    "        L = [{frozenset([item]): self._calculate_support([item])} for item in self.itemsets.keys()]\n",
    "        k = 2\n",
    "        while L:\n",
    "            Ck = self._join_sets(L, k)\n",
    "            L = self._prune(Ck)\n",
    "            if L:\n",
    "                self.freq_itemsets[k - 1] = L\n",
    "            k += 1\n",
    "\n",
    "    def _join_sets(self, itemsets, k):\n",
    "        joined_sets = []\n",
    "        for idx, itemset1 in enumerate(itemsets):\n",
    "            for jdx, itemset2 in enumerate(itemsets):\n",
    "                if jdx > idx:\n",
    "                    for set1 in itemset1:\n",
    "                        for set2 in itemset2:\n",
    "                            if isinstance(set1, str):\n",
    "                                set1 = {set1}\n",
    "                            if isinstance(set2, str):\n",
    "                                set2 = {set2}\n",
    "                            if set1 is not None and set2 is not None:  # Filter out None values\n",
    "                                joined_set = set1.union(set2)\n",
    "                                if len(joined_set) == k:\n",
    "                                    joined_sets.append(joined_set)\n",
    "        return joined_sets\n",
    "\n",
    "    def _prune(self, itemsets):\n",
    "        return [itemset for itemset in itemsets if self._calculate_support(itemset) >= self.min_support]\n",
    "\n",
    "    def _calculate_support(self, itemset):\n",
    "        count = 0\n",
    "        for transaction in self.transactions:\n",
    "            if all(item in transaction for item in itemset):\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def get_frequent_itemsets(self):\n",
    "        return self.freq_itemsets\n",
    "\n",
    "    def generate_association_rules(self, min_confidence):\n",
    "        rules = []\n",
    "        for itemset_length, itemsets in self.freq_itemsets.items():\n",
    "            if itemset_length < 2:\n",
    "                continue\n",
    "            for itemset in itemsets:\n",
    "                self._generate_rules_from_itemset(itemset, rules, min_confidence)\n",
    "        return rules\n",
    "\n",
    "    def _generate_rules_from_itemset(self, itemset, rules, min_confidence):\n",
    "        for subset in combinations(itemset, len(itemset) - 1):\n",
    "            antecedent = frozenset(subset)\n",
    "            consequent = itemset - antecedent\n",
    "            confidence = self._calculate_confidence(antecedent, consequent)\n",
    "            if confidence >= min_confidence:\n",
    "                rules.append((antecedent, consequent, confidence))\n",
    "\n",
    "    def _calculate_confidence(self, antecedent, consequent):\n",
    "        support_antecedent = self._calculate_support(antecedent)\n",
    "        support_itemset = self._calculate_support(antecedent.union(consequent))\n",
    "        return support_itemset / support_antecedent\n",
    "\n",
    "    def _generate_associations(self):\n",
    "        for itemset_length, itemsets in self.freq_itemsets.items():\n",
    "            if itemset_length < 2:\n",
    "                continue\n",
    "            for itemset in itemsets:\n",
    "                self._generate_associations_from_itemset(itemset)\n",
    "\n",
    "    def _generate_associations_from_itemset(self, itemset):\n",
    "        for subset_length in range(1, len(itemset)):\n",
    "            for subset in combinations(itemset, subset_length):\n",
    "                antecedent = frozenset(subset)\n",
    "                consequent = itemset - antecedent\n",
    "                self.associations.append((antecedent, consequent))\n",
    "\n",
    "# Load issues from JSON file\n",
    "with open('../flutter_30-issues.json', 'r', encoding='utf-8') as f:\n",
    "    issues = json.load(f)\n",
    "\n",
    "# Preprocessing: Extract relevant information\n",
    "transactions = []\n",
    "for issue in issues:\n",
    "    transaction = [str(issue[\"id\"]), issue[\"state_reason\"]]  # Convert items to strings\n",
    "    for label in issue[\"labels\"]:\n",
    "        transaction.append(label[\"name\"])\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Set minimum support and confidence\n",
    "min_support = 0.3\n",
    "min_confidence = 0.7\n",
    "\n",
    "# Apply ECLAT algorithm\n",
    "eclat = ECLAT(min_support)\n",
    "eclat.fit(transactions)\n",
    "\n",
    "# Get frequent itemsets and association rules\n",
    "freq_itemsets = eclat.get_frequent_itemsets()\n",
    "association_rules = eclat.generate_association_rules(min_confidence)\n",
    "associations = eclat.associations\n",
    "\n",
    "# Write results to CSV files\n",
    "with open('frequent_itemsets.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Itemset', 'Support']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for itemset_length, itemsets in freq_itemsets.items():\n",
    "        for itemset in itemsets:\n",
    "            if all(isinstance(item, str) for item in itemset):\n",
    "                writer.writerow({'Itemset': ','.join(map(str, itemset)), 'Support': eclat._calculate_support(itemset)})\n",
    "\n",
    "# Write association rules to CSV file\n",
    "with open('association_rules.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Antecedent', 'Consequent', 'Confidence']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for antecedent, consequent, confidence in association_rules:\n",
    "        writer.writerow({'Antecedent': ','.join(antecedent), 'Consequent': ','.join(consequent), 'Confidence': confidence})\n",
    "\n",
    "# Write associations to CSV file\n",
    "with open('associations.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Antecedent', 'Consequent']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for antecedent, consequent in associations:\n",
    "        writer.writerow({'Antecedent': ','.join(antecedent), 'Consequent': ','.join(consequent)})\n",
    "\n",
    "print(\"Results saved to frequent_itemsets.csv, association_rules.csv, and associations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ce8fb-cc0c-4f47-8d91-cdca2928d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ECLAT:\n",
    "    def __init__(self, min_support):\n",
    "        self.min_support = min_support\n",
    "        self.itemsets = {}\n",
    "        self.associations = []\n",
    "\n",
    "    def fit(self, transactions):\n",
    "        self.transactions = transactions\n",
    "        self._create_itemsets()\n",
    "        self._generate_associations()\n",
    "\n",
    "    def _create_itemsets(self):\n",
    "        for tid, transaction in enumerate(self.transactions):\n",
    "            for item in transaction:\n",
    "                if item in self.itemsets:\n",
    "                    self.itemsets[item].append(tid)\n",
    "                else:\n",
    "                    self.itemsets[item] = [tid]\n",
    "\n",
    "    def _generate_associations(self):\n",
    "        for item in self.itemsets.keys():\n",
    "            tid_list = self.itemsets[item]\n",
    "            for subset_length in range(1, len(tid_list)):\n",
    "                for subset in combinations(tid_list, subset_length):\n",
    "                    antecedent = frozenset(subset)\n",
    "                    consequent = frozenset(tid_list) - antecedent\n",
    "                    confidence = self._calculate_confidence(antecedent, consequent)\n",
    "                    self.associations.append((antecedent, consequent, confidence))\n",
    "\n",
    "    def _calculate_confidence(self, antecedent, consequent):\n",
    "        support_antecedent = self._calculate_support(antecedent)\n",
    "        if support_antecedent == 0:\n",
    "            return 0  # Return default confidence if support of antecedent is zero\n",
    "        support_itemset = self._calculate_support(antecedent.union(consequent))\n",
    "        return support_itemset / support_antecedent\n",
    "\n",
    "    def _calculate_support(self, itemset):\n",
    "        count = 0\n",
    "        for transaction in self.transactions:\n",
    "            if all(item in transaction for item in itemset):\n",
    "                count += 1\n",
    "        return count / len(self.transactions)\n",
    "\n",
    "# Load issues from JSON file\n",
    "with open('../flutter_30-issues.json', 'r', encoding='utf-8') as f:\n",
    "    issues = json.load(f)\n",
    "\n",
    "# Extract relevant attributes for analysis (e.g., reasons and issue types)\n",
    "transactions = []\n",
    "for issue in issues:\n",
    "    transaction = [issue[\"state_reason\"]]\n",
    "    for label in issue[\"labels\"]:\n",
    "        transaction.append(label[\"name\"])\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Set minimum support\n",
    "min_support = 0.1\n",
    "\n",
    "# Apply ECLAT algorithm\n",
    "eclat = ECLAT(min_support)\n",
    "eclat.fit(transactions)\n",
    "\n",
    "# Create a DataFrame to store association strength\n",
    "association_strength = pd.DataFrame(index=transactions[0], columns=transactions[0])\n",
    "\n",
    "# Calculate association strength between each attribute\n",
    "for antecedent, consequent, confidence in eclat.associations:\n",
    "    for a in antecedent:\n",
    "        for c in consequent:\n",
    "            association_strength.at[a, c] = max(confidence, association_strength.at[a, c])\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(association_strength.astype(float), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title('Association Heatmap between GitHub Issue Attributes')\n",
    "plt.xlabel('Consequent')\n",
    "plt.ylabel('Antecedent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe98f8-72f8-4d45-bc52-cfb9cb710de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
